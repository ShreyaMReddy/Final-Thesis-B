!pip install -U transformers torch tqdm scikit-learn
!pip install -U transformers==4.44.2 huggingface-hub==0.34.0
from transformers.models.deberta_v2 import DebertaV2Model, DebertaV2Tokenizer

tokenizer = DebertaV2Tokenizer.from_pretrained("microsoft/deberta-v3-base")
model = DebertaV2Model.from_pretrained("microsoft/deberta-v3-base")

!git clone https://github.com/jiayingwu19/SheepDog

%cd /content/SheepDog
!ls -la

import sys, os
sys.path.insert(0, os.getcwd())

from utils.load_data import *

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import DebertaV2Model, DebertaV2Tokenizer
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
import argparse
import numpy as np
import sys, os
sys.path.append(os.getcwd())
from utils.load_data import *
import warnings
from sklearn.metrics import precision_recall_fscore_support as score
from sklearn.metrics import accuracy_score
from tqdm import tqdm
warnings.filterwarnings("ignore")

parser = argparse.ArgumentParser()
parser.add_argument('--dataset_name', default='politifact', type=str)
parser.add_argument('--model_name', default='DeBERTa-v3', type=str)
parser.add_argument('--iters', default=5, type=int)
parser.add_argument('--batch_size', default=4, type=int)
parser.add_argument('--n_epochs', default=5, type=int)
args, unknown = parser.parse_known_args()

device = torch.device("cuda")

torch.manual_seed(0)
np.random.seed(0)
torch.backends.cudnn.deterministic = True
torch.cuda.manual_seed_all(0)


# ======================= DATASETS ======================= #
class NewsDatasetAug(Dataset):
    def __init__(self, texts, aug_texts1, aug_texts2, labels, fg_label, aug_fg1, aug_fg2, tokenizer, max_len):
        self.texts = texts
        self.aug_texts1 = aug_texts1
        self.aug_texts2 = aug_texts2
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.labels = labels
        self.fg_label = fg_label
        self.aug_fg1 = aug_fg1
        self.aug_fg2 = aug_fg2

    def __getitem__(self, item):
        text = self.texts[item]
        aug_text1 = self.aug_texts1[item]
        aug_text2 = self.aug_texts2[item]
        label = self.labels[item]
        fg_label = self.fg_label[item]
        aug_fg1 = self.aug_fg1[item]
        aug_fg2 = self.aug_fg2[item]

        encoding = self.tokenizer.encode_plus(
            text, add_special_tokens=True, max_length=self.max_len,
            padding='max_length', truncation=True,
            return_attention_mask=True, return_tensors='pt'
        )
        aug1_encoding = self.tokenizer.encode_plus(
            aug_text1, add_special_tokens=True, max_length=self.max_len,
            padding='max_length', truncation=True,
            return_attention_mask=True, return_tensors='pt'
        )
        aug2_encoding = self.tokenizer.encode_plus(
            aug_text2, add_special_tokens=True, max_length=self.max_len,
            padding='max_length', truncation=True,
            return_attention_mask=True, return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'input_ids_aug1': aug1_encoding['input_ids'].flatten(),
            'input_ids_aug2': aug2_encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'attention_mask_aug1': aug1_encoding['attention_mask'].flatten(),
            'attention_mask_aug2': aug2_encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long),
            'fg_label': torch.FloatTensor(fg_label),
            'fg_label_aug1': torch.FloatTensor(aug_fg1),
            'fg_label_aug2': torch.FloatTensor(aug_fg2),
        }

    def __len__(self):
        return len(self.texts)


class NewsDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __getitem__(self, item):
        text = self.texts[item]
        label = self.labels[item]
        encoding = self.tokenizer.encode_plus(
            text, add_special_tokens=True, max_length=self.max_len,
            padding='max_length', truncation=True,
            return_attention_mask=True, return_tensors='pt'
        )

        return {
            'news_text': text,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

    def __len__(self):
        return len(self.texts)


# ======================= MODEL ======================= #
class DebertaClassifier(nn.Module):
    def __init__(self, n_classes):
        super(DebertaClassifier, self).__init__()
        self.deberta = DebertaV2Model.from_pretrained('microsoft/deberta-v3-base')
        self.dropout = nn.Dropout(p=0.5)
        self.fc_out = nn.Linear(self.deberta.config.hidden_size, n_classes)
        self.binary_transform = nn.Linear(self.deberta.config.hidden_size, 2)

    def forward(self, input_ids, attention_mask):
        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token
        pooled_output = self.dropout(pooled_output)
        output = self.fc_out(pooled_output)
        binary_output = self.binary_transform(pooled_output)
        return output, binary_output


# ======================= LOADERS ======================= #
def create_train_loader(contents, contents_aug1, contents_aug2, labels, fg_label, aug_fg1, aug_fg2, tokenizer, max_len, batch_size):
    ds = NewsDatasetAug(texts=contents, aug_texts1=contents_aug1, aug_texts2=contents_aug2,
                        labels=np.array(labels), fg_label=fg_label, aug_fg1=aug_fg1, aug_fg2=aug_fg2,
                        tokenizer=tokenizer, max_len=max_len)
    return DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=2)


def create_eval_loader(contents, labels, tokenizer, max_len, batch_size):
    ds = NewsDataset(texts=contents, labels=np.array(labels), tokenizer=tokenizer, max_len=max_len)
    return DataLoader(ds, batch_size=batch_size, num_workers=0)


# ======================= UTILITIES ======================= #
def set_seed(seed):
    torch.manual_seed(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.cuda.manual_seed_all(seed)


import pickle


def load_articles(obj):
    print('Dataset:', obj)
    print("loading news articles")
    train_dict = pickle.load(open('data/news_articles/' + obj + '_train.pkl', 'rb'))
    test_dict = pickle.load(open('data/news_articles/' + obj + '_test.pkl', 'rb'))
    #
    restyle_dict = pickle.load(open('data/adversarial_test/' + obj + '_test_adv_A.pkl', 'rb'))

    x_train, y_train = train_dict['news'], train_dict['labels']
    x_test, y_test = test_dict['news'], test_dict['labels']
    x_test_res = restyle_dict['news']
    return x_train, x_test, x_test_res, y_train, y_test


def load_reframing(obj):
    print("loading news augmentations")
    print('Dataset:', obj)
    restyle_dict_train1_1 = pickle.load(open('data/reframings/' + obj + '_train_objective.pkl', 'rb'))
    restyle_dict_train1_2 = pickle.load(open('data/reframings/' + obj + '_train_neutral.pkl', 'rb'))
    restyle_dict_train2_1 = pickle.load(open('data/reframings/' + obj + '_train_emotionally_triggering.pkl', 'rb'))
    restyle_dict_train2_2 = pickle.load(open('data/reframings/' + obj + '_train_sensational.pkl', 'rb'))

    finegrain_dict1 = pickle.load(open('data/veracity_attributions/' + obj + '_fake_standards_objective_emotionally_triggering.pkl', 'rb'))
    finegrain_dict2 = pickle.load(open('data/veracity_attributions/' + obj + '_fake_standards_neutral_sensational.pkl', 'rb'))

    x_train_res1 = np.array(restyle_dict_train1_1['rewritten'])
    x_train_res1_2 = np.array(restyle_dict_train1_2['rewritten'])
    x_train_res2 = np.array(restyle_dict_train2_1['rewritten'])
    x_train_res2_2 = np.array(restyle_dict_train2_2['rewritten'])

    y_train_fg, y_train_fg_m, y_train_fg_t = finegrain_dict1['orig_fg'], finegrain_dict1['mainstream_fg'], finegrain_dict1['tabloid_fg']
    y_train_fg2, y_train_fg_m2, y_train_fg_t2 = finegrain_dict2['orig_fg'], finegrain_dict2['mainstream_fg'], finegrain_dict2['tabloid_fg']

    replace_idx = np.random.choice(len(x_train_res1), len(x_train_res1)//2, replace=False)
    x_train_res1[replace_idx] = x_train_res1_2[replace_idx]
    x_train_res2[replace_idx] = x_train_res2_2[replace_idx]
    y_train_fg[replace_idx] = y_train_fg2[replace_idx]
    y_train_fg_m[replace_idx] = y_train_fg_m2[replace_idx]
    y_train_fg_t[replace_idx] = y_train_fg_t2[replace_idx]

    return x_train_res1, x_train_res2, y_train_fg, y_train_fg_m, y_train_fg_t


# ======================= TRAINING ======================= #
def train_model(tokenizer, max_len, n_epochs, batch_size, datasetname, iter):
    x_train, x_test, x_test_res, y_train, y_test = load_articles(datasetname)
    test_loader = create_eval_loader(x_test, y_test, tokenizer, max_len, batch_size)
    test_loader_res = create_eval_loader(x_test_res, y_test, tokenizer, max_len, batch_size)

    model = DebertaClassifier(n_classes=4).to(device)
    optimizer = AdamW(model.parameters(), lr=2e-5)
    scheduler = get_linear_schedule_with_warmup(optimizer, 0, 10000)

    for epoch in range(n_epochs):
        model.train()
        x_train_res1, x_train_res2, y_train_fg, y_train_fg_m, y_train_fg_t = load_reframing(args.dataset_name)
        train_loader = create_train_loader(x_train, x_train_res1, x_train_res2, y_train, y_train_fg,
                                           y_train_fg_m, y_train_fg_t, tokenizer, max_len, batch_size)
        for Batch_data in tqdm(train_loader):
            input_ids = Batch_data["input_ids"].to(device)
            attention_mask = Batch_data["attention_mask"].to(device)
            input_ids_aug1 = Batch_data["input_ids_aug1"].to(device)
            attention_mask_aug1 = Batch_data["attention_mask_aug1"].to(device)
            input_ids_aug2 = Batch_data["input_ids_aug2"].to(device)
            attention_mask_aug2 = Batch_data["attention_mask_aug2"].to(device)
            targets = Batch_data["labels"].to(device)
            fg_labels = Batch_data["fg_label"].to(device)
            fg_labels_aug1 = Batch_data["fg_label_aug1"].to(device)
            fg_labels_aug2 = Batch_data["fg_label_aug2"].to(device)

            out_labels, out_labels_bi = model(input_ids, attention_mask)
            out_labels_aug1, out_labels_bi_aug1 = model(input_ids_aug1, attention_mask_aug1)
            out_labels_aug2, out_labels_bi_aug2 = model(input_ids_aug2, attention_mask_aug2)

            fg_loss = nn.BCELoss()(F.sigmoid(out_labels), fg_labels)
            fg_loss += nn.BCELoss()(F.sigmoid(out_labels_aug1), fg_labels_aug1)
            fg_loss += nn.BCELoss()(F.sigmoid(out_labels_aug2), fg_labels_aug2)
            fg_loss /= 3

            sup_loss = nn.CrossEntropyLoss()(out_labels_bi, targets)
            cons_loss = 0.5 * nn.KLDivLoss(reduction='batchmean')(
                F.log_softmax(out_labels_bi_aug1, dim=-1), F.softmax(out_labels_bi, dim=-1))
            cons_loss += 0.5 * nn.KLDivLoss(reduction='batchmean')(
                F.log_softmax(out_labels_bi_aug2, dim=-1), F.softmax(out_labels_bi, dim=-1))

            loss = sup_loss + cons_loss + fg_loss
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()
        print(f"Iter {iter:03d} | Epoch {epoch:05d} complete")

    # ======== EVAL ======== #
    model.eval()
    y_pred, y_pred_res, y_true = [], [], []
    for Batch_data in tqdm(test_loader):
        with torch.no_grad():
            ids, mask, targets = Batch_data["input_ids"].to(device), Batch_data["attention_mask"].to(device), Batch_data["labels"].to(device)
            _, logits = model(ids, mask)
            _, preds = logits.max(dim=1)
            y_pred.append(preds)
            y_true.append(targets)

    for Batch_data in tqdm(test_loader_res):
        with torch.no_grad():
            ids, mask = Batch_data["input_ids"].to(device), Batch_data["attention_mask"].to(device)
            _, logits = model(ids, mask)
            _, preds = logits.max(dim=1)
            y_pred_res.append(preds)

    y_pred, y_true, y_pred_res = torch.cat(y_pred), torch.cat(y_true), torch.cat(y_pred_res)
    acc = accuracy_score(y_true.cpu(), y_pred.cpu())
    precision, recall, fscore, _ = score(y_true.cpu(), y_pred.cpu(), average='macro')
    acc_res = accuracy_score(y_true.cpu(), y_pred_res.cpu())
    precision_res, recall_res, fscore_res, _ = score(y_true.cpu(), y_pred_res.cpu(), average='macro')

    torch.save(model.state_dict(), f'checkpoints/{datasetname}_iter{iter}.m')

    print("-----------------End of Iter {:03d}-----------------".format(iter))
    print(['Global Test Accuracy:{:.4f}'.format(acc),
           'Precision:{:.4f}'.format(precision),
           'Recall:{:.4f}'.format(recall),
           'F1:{:.4f}'.format(fscore)])
    print("-----------------Restyle-----------------")
    print(['Global Test Accuracy:{:.4f}'.format(acc_res),
           'Precision:{:.4f}'.format(precision_res),
           'Recall:{:.4f}'.format(recall_res),
           'F1:{:.4f}'.format(fscore_res)])

    return acc, precision, recall, fscore, acc_res, precision_res, recall_res, fscore_res


# ======================= MAIN ======================= #
datasetname = args.dataset_name
batch_size = args.batch_size
max_len = 512
tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-base')
n_epochs = args.n_epochs
iterations = args.iters

test_accs, prec_all, rec_all, f1_all = [], [], [], []
test_accs_res, prec_all_res, rec_all_res, f1_all_res = [], [], [], []

for iter in range(iterations):
    set_seed(iter)
    acc, prec, recall, f1, acc_res, prec_res, recall_res, f1_res = train_model(tokenizer, max_len, n_epochs, batch_size, datasetname, iter)
    test_accs.append(acc)
    prec_all.append(prec)
    rec_all.append(recall)
    f1_all.append(f1)
    test_accs_res.append(acc_res)
    prec_all_res.append(prec_res)
    rec_all_res.append(recall_res)
    f1_all_res.append(f1_res)

print("Total_Test_Accuracy: {:.4f}|Prec_Macro: {:.4f}|Rec_Macro: {:.4f}|F1_Macro: {:.4f}".format(
    np.mean(test_accs), np.mean(prec_all), np.mean(rec_all), np.mean(f1_all)))
print("Restyle_Test_Accuracy: {:.4f}|Prec_Macro: {:.4f}|Rec_Macro: {:.4f}|F1_Macro: {:.4f}".format(
    np.mean(test_accs_res), np.mean(prec_all_res), np.mean(rec_all_res), np.mean(f1_all_res)))

with open('logs/log_' + datasetname + '_' + args.model_name + '.' + 'iter' + str(iterations), 'a+') as f:
    f.write('-------------Original-------------\n')
    f.write(f'All Acc.s:{test_accs}\nAll Prec.s:{prec_all}\nAll Rec.s:{rec_all}\nAll F1.s:{f1_all}\n')
    f.write('Average acc.: {} \n'.format(np.mean(test_accs)))
    f.write('Average Prec / Rec / F1 (macro): {}, {}, {} \n'.format(np.mean(prec_all), np.mean(rec_all), np.mean(f1_all)))
    f.write('\n-------------Adversarial------------\n')
    f.write(f'All Acc.s:{test_accs_res}\nAll Prec.s:{prec_all_res}\nAll Rec.s:{rec_all_res}\nAll F1.s:{f1_all_res}\n')
    f.write('Average acc.: {} \n'.format(np.mean(test_accs_res)))
    f.write('Average Prec / Rec / F1 (macro): {}, {}, {} \n'.format(np.mean(prec_all_res), np.mean(rec_all_res), np.mean(f1_all_res)))

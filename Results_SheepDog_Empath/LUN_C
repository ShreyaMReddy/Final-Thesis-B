tokenizer_config.json: 100%
 25.0/25.0 [00:00<00:00, 3.11kB/s]
config.json: 100%
 481/481 [00:00<00:00, 69.0kB/s]
vocab.json: 100%
 899k/899k [00:00<00:00, 4.08MB/s]
merges.txt: 100%
 456k/456k [00:00<00:00, 2.09MB/s]
tokenizer.json: 100%
 1.36M/1.36M [00:00<00:00, 3.17MB/s]
Dataset:  lun
loading news articles
LIWC features extracted [[0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 ...
 [0.00151515 0.00151515 0.0030303  0.         0.        ]
 [0.         0.         0.00140647 0.         0.        ]
 [0.         0.         0.         0.         0.        ]]
model.safetensors: 100%
 499M/499M [00:01<00:00, 483MB/s]
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:50<00:00,  3.19it/s]
Iter 000 | Epoch 00000 | Train Acc. 0.9642 | Loss 0.9438
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:49<00:00,  3.19it/s]
Iter 000 | Epoch 00001 | Train Acc. 0.9910 | Loss 0.6277
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:49<00:00,  3.19it/s]
Iter 000 | Epoch 00002 | Train Acc. 0.9973 | Loss 0.4914
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:49<00:00,  3.20it/s]
Iter 000 | Epoch 00003 | Train Acc. 0.9987 | Loss 0.3965
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:50<00:00,  3.19it/s]
Iter 000 | Epoch 00004 | Train Acc. 0.9985 | Loss 0.3365
100%|██████████| 375/375 [00:11<00:00, 31.25it/s]
100%|██████████| 375/375 [00:11<00:00, 31.42it/s]
-----------------End of Iter 000-----------------
['Global Test Accuracy:0.9340', 'Precision:0.9360', 'Recall:0.9340', 'F1:0.9339']
-----------------Restyle-----------------
['Global Test Accuracy:0.8807', 'Precision:0.8807', 'Recall:0.8807', 'F1:0.8807']
Dataset:  lun
loading news articles
LIWC features extracted [[0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 ...
 [0.00151515 0.00151515 0.0030303  0.         0.        ]
 [0.         0.         0.00140647 0.         0.        ]
 [0.         0.         0.         0.         0.        ]]
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:51<00:00,  3.18it/s]
Iter 001 | Epoch 00000 | Train Acc. 0.9602 | Loss 0.9273
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:51<00:00,  3.18it/s]
Iter 001 | Epoch 00001 | Train Acc. 0.9913 | Loss 0.6272
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:51<00:00,  3.18it/s]
Iter 001 | Epoch 00002 | Train Acc. 0.9970 | Loss 0.4962
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:51<00:00,  3.18it/s]
Iter 001 | Epoch 00003 | Train Acc. 0.9987 | Loss 0.3945
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:51<00:00,  3.18it/s]
Iter 001 | Epoch 00004 | Train Acc. 0.9997 | Loss 0.3340
100%|██████████| 375/375 [00:12<00:00, 31.13it/s]
100%|██████████| 375/375 [00:11<00:00, 31.35it/s]
-----------------End of Iter 001-----------------
['Global Test Accuracy:0.9260', 'Precision:0.9265', 'Recall:0.9260', 'F1:0.9260']
-----------------Restyle-----------------
['Global Test Accuracy:0.8867', 'Precision:0.8867', 'Recall:0.8867', 'F1:0.8867']
Total_Test_Accuracy: 0.9300|Prec_Macro: 0.9312|Rec_Macro: 0.9300|F1_Macro: 0.9300
Restyle_Test_Accuracy: 0.8837|Prec_Macro: 0.8837|Rec_Macro: 0.8837|F1_Macro: 0.8837

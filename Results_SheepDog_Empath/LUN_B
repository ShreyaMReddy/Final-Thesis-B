tokenizer_config.json: 100%
 25.0/25.0 [00:00<00:00, 3.24kB/s]
config.json: 100%
 481/481 [00:00<00:00, 68.9kB/s]
vocab.json: 100%
 899k/899k [00:00<00:00, 4.50MB/s]
merges.txt: 100%
 456k/456k [00:00<00:00, 1.12MB/s]
tokenizer.json: 100%
 1.36M/1.36M [00:00<00:00, 20.9MB/s]
Dataset:  lun
loading news articles
LIWC features extracted [[0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 ...
 [0.00151515 0.00151515 0.0030303  0.         0.        ]
 [0.         0.         0.00140647 0.         0.        ]
 [0.         0.         0.         0.         0.        ]]
model.safetensors: 100%
 499M/499M [00:01<00:00, 355MB/s]
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:51<00:00,  3.18it/s]
Iter 000 | Epoch 00000 | Train Acc. 0.9642 | Loss 0.9438
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:51<00:00,  3.18it/s]
Iter 000 | Epoch 00001 | Train Acc. 0.9910 | Loss 0.6277
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:51<00:00,  3.18it/s]
Iter 000 | Epoch 00002 | Train Acc. 0.9973 | Loss 0.4914
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:52<00:00,  3.17it/s]
Iter 000 | Epoch 00003 | Train Acc. 0.9987 | Loss 0.3965
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:52<00:00,  3.17it/s]
Iter 000 | Epoch 00004 | Train Acc. 0.9985 | Loss 0.3365
100%|██████████| 375/375 [00:12<00:00, 31.15it/s]
100%|██████████| 375/375 [00:11<00:00, 31.30it/s]
-----------------End of Iter 000-----------------
['Global Test Accuracy:0.9340', 'Precision:0.9360', 'Recall:0.9340', 'F1:0.9339']
-----------------Restyle-----------------
['Global Test Accuracy:0.8747', 'Precision:0.8756', 'Recall:0.8747', 'F1:0.8746']
Dataset:  lun
loading news articles
LIWC features extracted [[0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 ...
 [0.00151515 0.00151515 0.0030303  0.         0.        ]
 [0.         0.         0.00140647 0.         0.        ]
 [0.         0.         0.         0.         0.        ]]
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:52<00:00,  3.17it/s]
Iter 001 | Epoch 00000 | Train Acc. 0.9602 | Loss 0.9273
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:52<00:00,  3.18it/s]
Iter 001 | Epoch 00001 | Train Acc. 0.9913 | Loss 0.6272
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:52<00:00,  3.17it/s]
Iter 001 | Epoch 00002 | Train Acc. 0.9970 | Loss 0.4962
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:52<00:00,  3.17it/s]
Iter 001 | Epoch 00003 | Train Acc. 0.9987 | Loss 0.3945
loading news augmentations
Dataset:  lun
100%|██████████| 1500/1500 [07:53<00:00,  3.17it/s]
Iter 001 | Epoch 00004 | Train Acc. 0.9997 | Loss 0.3340
100%|██████████| 375/375 [00:12<00:00, 31.16it/s]
100%|██████████| 375/375 [00:11<00:00, 31.34it/s]
-----------------End of Iter 001-----------------
['Global Test Accuracy:0.9260', 'Precision:0.9265', 'Recall:0.9260', 'F1:0.9260']
-----------------Restyle-----------------
['Global Test Accuracy:0.8587', 'Precision:0.8607', 'Recall:0.8587', 'F1:0.8585']
Total_Test_Accuracy: 0.9300|Prec_Macro: 0.9312|Rec_Macro: 0.9300|F1_Macro: 0.9300
Restyle_Test_Accuracy: 0.8667|Prec_Macro: 0.8682|Rec_Macro: 0.8667|F1_Macro: 0.8665

tokenizer_config.json: 100%
 25.0/25.0 [00:00<00:00, 3.32kB/s]
config.json: 100%
 481/481 [00:00<00:00, 65.6kB/s]
vocab.json: 100%
 899k/899k [00:00<00:00, 1.38MB/s]
merges.txt: 100%
 456k/456k [00:00<00:00, 1.04MB/s]
tokenizer.json: 100%
 1.36M/1.36M [00:00<00:00, 6.04MB/s]
Dataset:  gossipcop
loading news articles
LIWC features extracted [[0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.01111111 0.         0.         0.        ]
 ...
 [0.         0.         0.         0.         0.        ]
 [0.00372671 0.         0.00372671 0.         0.        ]
 [0.         0.         0.         0.         0.        ]]
model.safetensors: 100%
 499M/499M [00:01<00:00, 444MB/s]
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:24<00:00,  3.14it/s]
Iter 000 | Epoch 00000 | Train Acc. 0.7667 | Loss 1.4131
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:25<00:00,  3.13it/s]
Iter 000 | Epoch 00001 | Train Acc. 0.8358 | Loss 1.1455
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:25<00:00,  3.13it/s]
Iter 000 | Epoch 00002 | Train Acc. 0.8773 | Loss 0.9891
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:25<00:00,  3.13it/s]
Iter 000 | Epoch 00003 | Train Acc. 0.9217 | Loss 0.8091
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:24<00:00,  3.14it/s]
Iter 000 | Epoch 00004 | Train Acc. 0.9526 | Loss 0.6644
100%|██████████| 396/396 [00:13<00:00, 28.31it/s]
100%|██████████| 396/396 [00:12<00:00, 31.40it/s]
-----------------End of Iter 000-----------------
['Global Test Accuracy:0.7544', 'Precision:0.7549', 'Recall:0.7544', 'F1:0.7543']
-----------------Restyle-----------------
['Global Test Accuracy:0.7506', 'Precision:0.7517', 'Recall:0.7506', 'F1:0.7504']
Dataset:  gossipcop
loading news articles
LIWC features extracted [[0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.01111111 0.         0.         0.        ]
 ...
 [0.         0.         0.         0.         0.        ]
 [0.00372671 0.         0.00372671 0.         0.        ]
 [0.         0.         0.         0.         0.        ]]
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:27<00:00,  3.12it/s]
Iter 001 | Epoch 00000 | Train Acc. 0.7644 | Loss 1.4126
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:27<00:00,  3.12it/s]
Iter 001 | Epoch 00001 | Train Acc. 0.8315 | Loss 1.1464
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:27<00:00,  3.12it/s]
Iter 001 | Epoch 00002 | Train Acc. 0.8834 | Loss 0.9559
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:29<00:00,  3.11it/s]
Iter 001 | Epoch 00003 | Train Acc. 0.9253 | Loss 0.7846
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:30<00:00,  3.10it/s]
Iter 001 | Epoch 00004 | Train Acc. 0.9558 | Loss 0.6315
100%|██████████| 396/396 [00:14<00:00, 28.11it/s]
100%|██████████| 396/396 [00:12<00:00, 31.39it/s]
-----------------End of Iter 001-----------------
['Global Test Accuracy:0.7544', 'Precision:0.7555', 'Recall:0.7544', 'F1:0.7542']
-----------------Restyle-----------------
['Global Test Accuracy:0.7557', 'Precision:0.7567', 'Recall:0.7557', 'F1:0.7554']
Total_Test_Accuracy: 0.7544|Prec_Macro: 0.7552|Rec_Macro: 0.7544|F1_Macro: 0.7542
Restyle_Test_Accuracy: 0.7532|Prec_Macro: 0.7542|Rec_Macro: 0.7532|F1_Macro: 0.7529

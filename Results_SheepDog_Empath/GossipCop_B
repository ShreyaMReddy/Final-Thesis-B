tokenizer_config.json: 100%
 25.0/25.0 [00:00<00:00, 3.10kB/s]
config.json: 100%
 481/481 [00:00<00:00, 65.3kB/s]
vocab.json: 100%
 899k/899k [00:00<00:00, 2.00MB/s]
merges.txt: 100%
 456k/456k [00:00<00:00, 25.2MB/s]
tokenizer.json: 100%
 1.36M/1.36M [00:00<00:00, 2.10MB/s]
Dataset:  gossipcop
loading news articles
LIWC features extracted [[0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.01111111 0.         0.         0.        ]
 ...
 [0.         0.         0.         0.         0.        ]
 [0.00372671 0.         0.00372671 0.         0.        ]
 [0.         0.         0.         0.         0.        ]]
model.safetensors: 100%
 499M/499M [00:02<00:00, 348MB/s]
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:15<00:00,  3.19it/s]
Iter 000 | Epoch 00000 | Train Acc. 0.7667 | Loss 1.4131
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:16<00:00,  3.19it/s]
Iter 000 | Epoch 00001 | Train Acc. 0.8358 | Loss 1.1455
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:16<00:00,  3.19it/s]
Iter 000 | Epoch 00002 | Train Acc. 0.8773 | Loss 0.9891
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:15<00:00,  3.19it/s]
Iter 000 | Epoch 00003 | Train Acc. 0.9217 | Loss 0.8091
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:15<00:00,  3.19it/s]
Iter 000 | Epoch 00004 | Train Acc. 0.9526 | Loss 0.6644
100%|██████████| 396/396 [00:14<00:00, 28.17it/s]
100%|██████████| 396/396 [00:12<00:00, 31.91it/s]
-----------------End of Iter 000-----------------
['Global Test Accuracy:0.7544', 'Precision:0.7549', 'Recall:0.7544', 'F1:0.7543']
-----------------Restyle-----------------
['Global Test Accuracy:0.7462', 'Precision:0.7470', 'Recall:0.7462', 'F1:0.7460']
Dataset:  gossipcop
loading news articles
LIWC features extracted [[0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.01111111 0.         0.         0.        ]
 ...
 [0.         0.         0.         0.         0.        ]
 [0.00372671 0.         0.00372671 0.         0.        ]
 [0.         0.         0.         0.         0.        ]]
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:17<00:00,  3.18it/s]
Iter 001 | Epoch 00000 | Train Acc. 0.7644 | Loss 1.4126
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:18<00:00,  3.18it/s]
Iter 001 | Epoch 00001 | Train Acc. 0.8315 | Loss 1.1464
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:18<00:00,  3.18it/s]
Iter 001 | Epoch 00002 | Train Acc. 0.8834 | Loss 0.9559
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:18<00:00,  3.18it/s]
Iter 001 | Epoch 00003 | Train Acc. 0.9253 | Loss 0.7846
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:18<00:00,  3.17it/s]
Iter 001 | Epoch 00004 | Train Acc. 0.9558 | Loss 0.6315
100%|██████████| 396/396 [00:14<00:00, 28.14it/s]
100%|██████████| 396/396 [00:12<00:00, 31.97it/s]
-----------------End of Iter 001-----------------
['Global Test Accuracy:0.7544', 'Precision:0.7555', 'Recall:0.7544', 'F1:0.7542']
-----------------Restyle-----------------
['Global Test Accuracy:0.7418', 'Precision:0.7421', 'Recall:0.7418', 'F1:0.7417']
Total_Test_Accuracy: 0.7544|Prec_Macro: 0.7552|Rec_Macro: 0.7544|F1_Macro: 0.7542
Restyle_Test_Accuracy: 0.7440|Prec_Macro: 0.7445|Rec_Macro: 0.7440|F1_Macro: 0.7439

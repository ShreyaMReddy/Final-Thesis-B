tokenizer_config.json: 100%
 25.0/25.0 [00:00<00:00, 3.11kB/s]
config.json: 100%
 481/481 [00:00<00:00, 59.6kB/s]
vocab.json: 100%
 899k/899k [00:00<00:00, 3.65MB/s]
merges.txt: 100%
 456k/456k [00:00<00:00, 27.2MB/s]
tokenizer.json: 100%
 1.36M/1.36M [00:00<00:00, 4.19MB/s]
Dataset:  gossipcop
loading news articles
LIWC features extracted [[0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.01111111 0.         0.         0.        ]
 ...
 [0.         0.         0.         0.         0.        ]
 [0.00372671 0.         0.00372671 0.         0.        ]
 [0.         0.         0.         0.         0.        ]]
model.safetensors: 100%
 499M/499M [00:01<00:00, 604MB/s]
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:19<00:00,  3.17it/s]
Iter 000 | Epoch 00000 | Train Acc. 0.7667 | Loss 1.4131
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:20<00:00,  3.16it/s]
Iter 000 | Epoch 00001 | Train Acc. 0.8358 | Loss 1.1455
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:20<00:00,  3.16it/s]
Iter 000 | Epoch 00002 | Train Acc. 0.8773 | Loss 0.9891
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:20<00:00,  3.16it/s]
Iter 000 | Epoch 00003 | Train Acc. 0.9217 | Loss 0.8091
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:20<00:00,  3.16it/s]
Iter 000 | Epoch 00004 | Train Acc. 0.9526 | Loss 0.6644
100%|██████████| 396/396 [00:14<00:00, 27.84it/s]
100%|██████████| 396/396 [00:12<00:00, 31.62it/s]
-----------------End of Iter 000-----------------
['Global Test Accuracy:0.7544', 'Precision:0.7549', 'Recall:0.7544', 'F1:0.7543']
-----------------Restyle-----------------
['Global Test Accuracy:0.7494', 'Precision:0.7505', 'Recall:0.7494', 'F1:0.7491']
Dataset:  gossipcop
loading news articles
LIWC features extracted [[0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.01111111 0.         0.         0.        ]
 ...
 [0.         0.         0.         0.         0.        ]
 [0.00372671 0.         0.00372671 0.         0.        ]
 [0.         0.         0.         0.         0.        ]]
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:22<00:00,  3.15it/s]
Iter 001 | Epoch 00000 | Train Acc. 0.7644 | Loss 1.4126
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:23<00:00,  3.14it/s]
Iter 001 | Epoch 00001 | Train Acc. 0.8315 | Loss 1.1464
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:23<00:00,  3.14it/s]
Iter 001 | Epoch 00002 | Train Acc. 0.8834 | Loss 0.9559
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:23<00:00,  3.14it/s]
Iter 001 | Epoch 00003 | Train Acc. 0.9253 | Loss 0.7846
loading news augmentations
Dataset:  gossipcop
100%|██████████| 1583/1583 [08:23<00:00,  3.14it/s]
Iter 001 | Epoch 00004 | Train Acc. 0.9558 | Loss 0.6315
100%|██████████| 396/396 [00:15<00:00, 25.80it/s]
100%|██████████| 396/396 [00:12<00:00, 31.81it/s]
-----------------End of Iter 001-----------------
['Global Test Accuracy:0.7544', 'Precision:0.7555', 'Recall:0.7544', 'F1:0.7542']
-----------------Restyle-----------------
['Global Test Accuracy:0.7563', 'Precision:0.7573', 'Recall:0.7563', 'F1:0.7561']
Total_Test_Accuracy: 0.7544|Prec_Macro: 0.7552|Rec_Macro: 0.7544|F1_Macro: 0.7542
Restyle_Test_Accuracy: 0.7528|Prec_Macro: 0.7539|Rec_Macro: 0.7528|F1_Macro: 0.7526

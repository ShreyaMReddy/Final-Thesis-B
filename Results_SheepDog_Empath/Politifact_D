tokenizer_config.json: 100%
 25.0/25.0 [00:00<00:00, 2.42kB/s]
config.json: 100%
 481/481 [00:00<00:00, 47.3kB/s]
vocab.json: 100%
 899k/899k [00:00<00:00, 18.8MB/s]
merges.txt: 100%
 456k/456k [00:00<00:00, 36.1MB/s]
tokenizer.json: 100%
 1.36M/1.36M [00:00<00:00, 5.86MB/s]
Dataset:  politifact
loading news articles
LIWC features extracted [[0.         0.         0.00209644 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.00209644 0.         0.        ]
 [0.00079681 0.         0.         0.         0.        ]
 [0.00054935 0.00036623 0.00256363 0.         0.        ]
 [0.00035608 0.         0.00077151 0.         0.        ]
 [0.00274725 0.         0.00274725 0.         0.        ]
 [0.00024432 0.00024432 0.00024432 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.00121065 0.         0.        ]
 [0.         0.00046591 0.         0.         0.        ]
 [0.0001227  0.         0.0007362  0.         0.        ]
 [0.00032199 0.00042932 0.00042932 0.         0.        ]
 [0.         0.00030271 0.00060542 0.         0.        ]
 [0.00034746 0.         0.00069493 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00018839 0.00037679 0.         0.         0.        ]
 [0.00173385 0.         0.00346771 0.         0.        ]
 [0.00055835 0.         0.00055835 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00178891 0.00357782 0.00536673 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.00236407 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00145773 0.00087464 0.00145773 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00012464 0.00024928 0.00037392 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.00408719 0.         0.        ]
 [0.         0.00316456 0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.00083647 0.00041824 0.         0.        ]
 [0.         0.         0.0009901  0.         0.        ]
 [0.         0.00373134 0.00373134 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.00550964 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00263158 0.00131579 0.00394737 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00193986 0.         0.         0.         0.        ]
 [0.0013986  0.         0.0013986  0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00113379 0.         0.00453515 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00364964 0.         0.00364964 0.         0.        ]
 [0.0004948  0.         0.00148441 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.00662252 0.         0.         0.        ]
 [0.00161812 0.         0.         0.         0.        ]
 [0.         0.00126422 0.00126422 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00358423 0.         0.00358423 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.00108342 0.         0.        ]
 [0.         0.         0.00352113 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00104275 0.         0.00521376 0.         0.        ]
 [0.         0.00054171 0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.0008244  0.         0.0016488  0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.00230415 0.         0.        ]
 [0.         0.         0.00403226 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00220264 0.         0.00220264 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00508906 0.         0.00254453 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.00160514 0.         0.        ]
 [0.00109709 0.         0.00438837 0.         0.        ]]
model.safetensors: 100%
 499M/499M [00:01<00:00, 486MB/s]
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading news augmentations
Dataset:  politifact
100%|██████████| 90/90 [00:29<00:00,  3.01it/s]
Iter 000 | Epoch 00000 | Train Acc. 0.7194 | Loss 1.8232
loading news augmentations
Dataset:  politifact
100%|██████████| 90/90 [00:29<00:00,  3.04it/s]
Iter 000 | Epoch 00001 | Train Acc. 0.9222 | Loss 1.1772
loading news augmentations
Dataset:  politifact
100%|██████████| 90/90 [00:29<00:00,  3.04it/s]
Iter 000 | Epoch 00002 | Train Acc. 0.9500 | Loss 0.9765
loading news augmentations
Dataset:  politifact
100%|██████████| 90/90 [00:29<00:00,  3.04it/s]
Iter 000 | Epoch 00003 | Train Acc. 0.9806 | Loss 0.7740
loading news augmentations
Dataset:  politifact
100%|██████████| 90/90 [00:29<00:00,  3.03it/s]
Iter 000 | Epoch 00004 | Train Acc. 0.9750 | Loss 0.7359
100%|██████████| 23/23 [00:01<00:00, 22.46it/s]
100%|██████████| 23/23 [00:00<00:00, 32.25it/s]
-----------------End of Iter 000-----------------
['Global Test Accuracy:0.8778', 'Precision:0.8795', 'Recall:0.8778', 'F1:0.8776']
-----------------Restyle-----------------
['Global Test Accuracy:0.8111', 'Precision:0.8125', 'Recall:0.8111', 'F1:0.8109']
Dataset:  politifact
loading news articles
LIWC features extracted [[0.         0.         0.00209644 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.00209644 0.         0.        ]
 [0.00079681 0.         0.         0.         0.        ]
 [0.00054935 0.00036623 0.00256363 0.         0.        ]
 [0.00035608 0.         0.00077151 0.         0.        ]
 [0.00274725 0.         0.00274725 0.         0.        ]
 [0.00024432 0.00024432 0.00024432 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.00121065 0.         0.        ]
 [0.         0.00046591 0.         0.         0.        ]
 [0.0001227  0.         0.0007362  0.         0.        ]
 [0.00032199 0.00042932 0.00042932 0.         0.        ]
 [0.         0.00030271 0.00060542 0.         0.        ]
 [0.00034746 0.         0.00069493 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00018839 0.00037679 0.         0.         0.        ]
 [0.00173385 0.         0.00346771 0.         0.        ]
 [0.00055835 0.         0.00055835 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00178891 0.00357782 0.00536673 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.00236407 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00145773 0.00087464 0.00145773 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00012464 0.00024928 0.00037392 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.00408719 0.         0.        ]
 [0.         0.00316456 0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.00083647 0.00041824 0.         0.        ]
 [0.         0.         0.0009901  0.         0.        ]
 [0.         0.00373134 0.00373134 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.00550964 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00263158 0.00131579 0.00394737 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00193986 0.         0.         0.         0.        ]
 [0.0013986  0.         0.0013986  0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00113379 0.         0.00453515 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00364964 0.         0.00364964 0.         0.        ]
 [0.0004948  0.         0.00148441 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.00662252 0.         0.         0.        ]
 [0.00161812 0.         0.         0.         0.        ]
 [0.         0.00126422 0.00126422 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00358423 0.         0.00358423 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.00108342 0.         0.        ]
 [0.         0.         0.00352113 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00104275 0.         0.00521376 0.         0.        ]
 [0.         0.00054171 0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.0008244  0.         0.0016488  0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.00230415 0.         0.        ]
 [0.         0.         0.00403226 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00220264 0.         0.00220264 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.00508906 0.         0.00254453 0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.         0.         0.00160514 0.         0.        ]
 [0.00109709 0.         0.00438837 0.         0.        ]]
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading news augmentations
Dataset:  politifact
100%|██████████| 90/90 [00:29<00:00,  3.03it/s]
Iter 001 | Epoch 00000 | Train Acc. 0.7167 | Loss 1.8727
loading news augmentations
Dataset:  politifact
100%|██████████| 90/90 [00:29<00:00,  3.03it/s]
Iter 001 | Epoch 00001 | Train Acc. 0.8944 | Loss 1.2726
loading news augmentations
Dataset:  politifact
100%|██████████| 90/90 [00:29<00:00,  3.03it/s]
Iter 001 | Epoch 00002 | Train Acc. 0.9444 | Loss 0.9497
loading news augmentations
Dataset:  politifact
100%|██████████| 90/90 [00:29<00:00,  3.03it/s]
Iter 001 | Epoch 00003 | Train Acc. 0.9667 | Loss 0.8240
loading news augmentations
Dataset:  politifact
100%|██████████| 90/90 [00:29<00:00,  3.03it/s]
Iter 001 | Epoch 00004 | Train Acc. 0.9833 | Loss 0.7082
100%|██████████| 23/23 [00:01<00:00, 22.64it/s]
100%|██████████| 23/23 [00:00<00:00, 32.27it/s]
-----------------End of Iter 001-----------------
['Global Test Accuracy:0.8667', 'Precision:0.8674', 'Recall:0.8667', 'F1:0.8666']
-----------------Restyle-----------------
['Global Test Accuracy:0.8000', 'Precision:0.8024', 'Recall:0.8000', 'F1:0.7996']
Total_Test_Accuracy: 0.8722|Prec_Macro: 0.8734|Rec_Macro: 0.8722|F1_Macro: 0.8721
Restyle_Test_Accuracy: 0.8056|Prec_Macro: 0.8074|Rec_Macro: 0.8056|F1_Macro: 0.8053
